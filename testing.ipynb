{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self,\n",
    "                mode,\n",
    "                data_path, \n",
    "                meta_train_devices,\n",
    "                meta_valid_devices, \n",
    "                meta_test_devices,\n",
    "                num_inner_tasks, \n",
    "                num_meta_train_sample,\n",
    "                num_sample, \n",
    "                num_query,\n",
    "                num_query_meta_train_task=200,\n",
    "                remove_outlier=True):\n",
    "        self.mode = mode\n",
    "        self.data_path = data_path\n",
    "        self.meta_train_devices = meta_train_devices\n",
    "        self.meta_valid_devices = meta_valid_devices\n",
    "        self.meta_test_devices = meta_test_devices\n",
    "        self.num_inner_tasks = num_inner_tasks\n",
    "        self.num_meta_train_sample = num_meta_train_sample\n",
    "        self.num_sample = num_sample\n",
    "        self.num_query = num_query\n",
    "        self.num_query_meta_train_task = num_query_meta_train_task\n",
    "        self.remove_outlier = remove_outlier\n",
    "        \n",
    "        self.load_archs()\n",
    "\n",
    "        self.train_idx ={}\n",
    "        self.valid_idx = {}\n",
    "        self.latency = {}\n",
    "        self.norm_latency = {}\n",
    "        nts = self.num_meta_train_sample\n",
    "        for device in meta_train_devices + meta_valid_devices + meta_test_devices:\n",
    "            self.latency[device] = torch.FloatTensor(\n",
    "                torch.load(os.path.join(data_path, 'latency', f'{device}.pt')))\n",
    "            train_idx = torch.arange(len(self.archs))[:nts]\n",
    "            valid_idx = torch.arange(len(self.archs))[nts:nts+self.num_query]\n",
    "\n",
    "            if self.remove_outlier:\n",
    "                self.train_idx[device] = train_idx[\n",
    "                    np.argsort(self.latency[device][train_idx])[\n",
    "                        int(len(train_idx)*0.1):int(len(train_idx)*0.9)]]\n",
    "                self.valid_idx[device] = valid_idx[\n",
    "                    np.argsort(self.latency[device][valid_idx])[\n",
    "                        int(len(valid_idx)*0.1):int(len(valid_idx)*0.9)]]\n",
    "\n",
    "            self.norm_latency[device] = normalization(\n",
    "                                        latency=self.latency[device],\n",
    "                                        index = self.train_idx[device]\n",
    "                                        )\n",
    "        # load index set of reference architectures\n",
    "        self.hw_emb_idx = torch.load(\n",
    "            os.path.join(data_path, f'{self.head_or_backbone}_hardware_embedding_index.pt'))\n",
    "\n",
    "        if self.mode == 'nas':\n",
    "            self.max_lat_idx, self.min_lat_idx = get_minmax_latency_index(\n",
    "                meta_train_devices + meta_valid_devices, self.train_idx, self.latency)\n",
    "            self.nas_norm_latency = {}\n",
    "            for device in meta_train_devices + meta_valid_devices + meta_test_devices:\n",
    "                self.nas_norm_latency[device] = normalization(\n",
    "                    latency=self.latency[device],\n",
    "                    index = torch.tensor([self.max_lat_idx, self.min_lat_idx] + self.hw_emb_idx))\n",
    "\n",
    "        print('==> load data ...')\n",
    "\n",
    "\n",
    "    def load_archs(self):\n",
    "        head_or_backbone = self.meta_train_devices[0].split('_')[-1] # 'head' or 'backbone'\n",
    "        if head_or_backbone not in ['head', 'backbone']: raise ValueError('Device 이름은 \"_head\"나 \"_backbone\"으로 끝나야 합니다.')\n",
    "\n",
    "        max_depth = 5 if head_or_backbone == 'backbone' else 7\n",
    "        self.head_or_backbone = head_or_backbone\n",
    "\n",
    "        self.archs = [arch_encoding_ofa(arch, max_depth) for arch in \n",
    "            torch.load(os.path.join(self.data_path, f'{head_or_backbone}_archs.pt'))['arch']]\n",
    "\n",
    "\n",
    "    def generate_episode(self):\n",
    "        # metabatch\n",
    "        episode = []\n",
    "\n",
    "        # meta-batch\n",
    "        rand_device_idx = torch.randperm(\n",
    "                            len(self.meta_train_devices))[:self.num_inner_tasks]\n",
    "        for t in rand_device_idx:\n",
    "            # sample devices\n",
    "            device = self.meta_train_devices[t]\n",
    "            # hardware embedding\n",
    "            latency = self.latency[device]\n",
    "            hw_embed = latency[self.hw_emb_idx]\n",
    "            hw_embed = normalization(hw_embed, portion=1.0)\n",
    "\n",
    "            # samples for finetuning & test (query)\n",
    "            rand_idx = self.train_idx[device][torch.randperm(len(self.train_idx[device]))]\n",
    "            finetune_idx = rand_idx[:self.num_sample]\n",
    "            qry_idx = rand_idx[self.num_sample:self.num_sample+self.num_query_meta_train_task]\n",
    "\n",
    "            x_finetune = torch.stack([self.archs[_] for _ in finetune_idx])\n",
    "            x_qry = torch.stack([self.archs[_] for _ in qry_idx])\n",
    "\n",
    "            y_finetune = self.norm_latency[device][finetune_idx].view(-1, 1)\n",
    "            y_qry = self.norm_latency[device][qry_idx].view(-1, 1)\n",
    "\n",
    "            episode.append((hw_embed, x_finetune, y_finetune, x_qry, y_qry, device))\n",
    "\n",
    "        return episode\n",
    "\n",
    "    def generate_test_tasks(self, split=None):\n",
    "        if split == 'meta_train':\n",
    "            device_list = self.meta_train_devices\n",
    "        elif split == 'meta_valid':\n",
    "            device_list = self.meta_valid_devices\n",
    "        elif split == 'meta_test':\n",
    "            device_list = self.meta_test_devices\n",
    "        else: NotImplementedError\n",
    "\n",
    "        tasks = []\n",
    "        for device in device_list:\n",
    "            tasks.append(self.get_task(device))\n",
    "        return tasks\n",
    "\n",
    "    def get_task(self, device=None, num_sample=None):\n",
    "        if num_sample == None:\n",
    "            num_sample = self.num_sample\n",
    "    \n",
    "        latency = self.latency[device]\n",
    "        # hardware embedding\n",
    "        hw_embed = latency[self.hw_emb_idx]\n",
    "        hw_embed = normalization(hw_embed, portion=1.0)        \n",
    "        \n",
    "        # samples for finetuing & test (query)\n",
    "        rand_idx = self.train_idx[device][torch.randperm(len(self.train_idx[device]))]\n",
    "        finetune_idx = rand_idx[:num_sample]\n",
    "\n",
    "        x_finetune = torch.stack([self.archs[_] for _ in finetune_idx])\n",
    "        x_qry = torch.stack([self.archs[_] for _ in self.valid_idx[device]])\n",
    "        \n",
    "        y_finetune = self.norm_latency[device][finetune_idx].view(-1, 1)\n",
    "        y_qry = self.norm_latency[device][self.valid_idx[device]].view(-1, 1)\n",
    "\n",
    "        return hw_embed, x_finetune, y_finetune, x_qry, y_qry, device\n",
    "\n",
    "\n",
    "    def get_nas_task(self, device=None):\n",
    "        latency = self.latency[device]\n",
    "        # hardware embedding\n",
    "        hw_embed = latency[self.hw_emb_idx]\n",
    "        hw_embed = normalization(hw_embed, portion=1.0)        \n",
    "\n",
    "        # samples for finetuning & test (query)\n",
    "        finetune_idx = self.hw_emb_idx\n",
    "        norm_latency = self.nas_norm_latency[device]\n",
    "        x_finetune = torch.stack([self.archs[_] for _ in finetune_idx])\n",
    "\n",
    "        y_finetune = norm_latency[finetune_idx].view(-1, 1)\n",
    "        y_finetune_gt = latency[finetune_idx].view(-1, 1)\n",
    "\n",
    "        return hw_embed, x_finetune, y_finetune, y_finetune_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> load data ...\n"
     ]
    }
   ],
   "source": [
    "data = Data(\n",
    "    mode='meta-train',\n",
    "    data_path='data', \n",
    "    meta_train_devices=['galaxy_s10_cpu_head', 'galaxy_s10_gpu_head', 'galaxy_s22_nnapi_head'],\n",
    "    meta_valid_devices=['galaxy_s22_cpu_head', 'galaxy_s22_gpu_head'], \n",
    "    meta_test_devices=['galaxy_s10_nnapi_head'],\n",
    "    num_inner_tasks=8,\n",
    "    num_meta_train_sample=900,\n",
    "    num_sample=10,\n",
    "    num_query=1000,\n",
    "    num_query_meta_train_task=200,\n",
    "    remove_outlier=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi = data.generate_episode()\n",
    "len(epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
