{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Pytorch → onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userHome/userhome1/hanyong/yolov7-nas-for-TANGO/nas/supernet/dynamic_layers/dynamic_op.py:246: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if bn.num_features == feature_dim or DynamicBatchNorm2d.SET_RUNNING_STATISTICS:\n",
      "/userHome/userhome1/hanyong/miniconda3/envs/openvivo/lib/python3.7/site-packages/torch/onnx/symbolic_helper.py:821: UserWarning: You are trying to export the model with onnx:Resize for ONNX opset version 10. This operator might cause results to not match the expected results by PyTorch.\n",
      "ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n",
      "We recommend using opset 11 and above for models using this operator.\n",
      "  \"\" + str(GLOBALS.export_onnx_opset_version) + \". \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from latency_predictor.arch_utils import *\n",
    "\n",
    "from nas.supernet.supernet_yolov7 import YOLOSuperNet\n",
    "\n",
    "device = 'cpu'\n",
    "supernet = YOLOSuperNet(cfg='yaml_config/yolov7_supernet.yml').to(device)\n",
    "\n",
    "supernet.set_active_subnet([5,5,5,5,7,7,7,1])\n",
    "subnet = supernet.get_active_subnet()\n",
    "b_arch, h_arch, save = split_backbone_head(subnet)\n",
    "backbone = skin_backbone(b_arch, save).eval()\n",
    "head = skin_head(h_arch, save).eval()\n",
    "\n",
    "img = torch.rand(1, 3, 640, 640).to(device)\n",
    "torch.onnx.export(backbone,\n",
    "                  img,\n",
    "                  'exported_models/backbone_5555.onnx',\n",
    "                  export_params=True,\n",
    "                  verbose=False,\n",
    "                  opset_version=10,        \n",
    "                  do_constant_folding=False,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'])\n",
    "\n",
    "img = torch.rand(1, 1024, 20, 20).to(device)\n",
    "torch.onnx.export(head,\n",
    "                  img,\n",
    "                  'exported_models/head_7771.onnx',\n",
    "                  export_params=True,\n",
    "                  verbose=False,\n",
    "                  opset_version=10,\n",
    "                  do_constant_folding=False,\n",
    "                  input_names = ['input'],\n",
    "                  output_names = ['output'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. onnx → onnx-simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "onnx_model_paths = [\n",
    "    'exported_models/backbone_5555.onnx',\n",
    "    'exported_models/head_7771.onnx'\n",
    "]\n",
    "\n",
    "for path in onnx_model_paths:\n",
    "    onnx_model = onnx.load(path)\n",
    "\n",
    "    # convert model\n",
    "    model_simp, check = simplify(onnx_model)\n",
    "\n",
    "    assert check, \"Simplified ONNX model could not be validated\"\n",
    "    onnx.save(model_simp, path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. onnx-simplify → OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Generated IR will be compressed to FP16. If you get lower accuracy, please consider disabling compression by removing argument --compress_to_fp16 or set it to false --compress_to_fp16=False.\n",
      "Find more information about compression to FP16 at https://docs.openvino.ai/latest/openvino_docs_MO_DG_FP16_Compression.html\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: /userHome/userhome1/hanyong/yolov7-nas-for-TANGO/exported_models/openvino/head_7_7_7_1.xml\n",
      "[ SUCCESS ] BIN file: /userHome/userhome1/hanyong/yolov7-nas-for-TANGO/exported_models/openvino/head_7_7_7_1.bin\n"
     ]
    }
   ],
   "source": [
    "!mo --input_model \"../exported_models/head_7771.onnx\" --compress_to_fp16 --output_dir \"../exported_models/openvino\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. OpenVINO → tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker pull ghcr.io/pinto0309/openvino2tensorflow:latest\n",
    "\n",
    "docker run -it --rm \\\n",
    "  -v `pwd`:/home/user/workdir \\\n",
    "  ghcr.io/pinto0309/openvino2tensorflow:latest\n",
    "\n",
    "MODEL_NAME=head_7771\n",
    "MODEL_PATH=exported_models/openvino/${MODEL_NAME}.xml\n",
    "OUTPUT_PATH=exported_models/openvino/tflite/${MODEL_NAME}\n",
    "\n",
    "sudo openvino2tensorflow --model_path $MODEL_PATH --model_output_path $OUTPUT_PATH --output_saved_model --output_float16_quant_tflite\n",
    "\n",
    "sudo mv ${OUTPUT_PATH}/model_float16_quant.tflite ${OUTPUT_PATH}_f16.tflite\n",
    "sudo rm -rf $OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2tflite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
